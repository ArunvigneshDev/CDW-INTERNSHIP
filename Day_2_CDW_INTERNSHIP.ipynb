{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7cyenodqafN",
        "outputId": "37068e14-bf7c-4b3b-b04f-039a7f97c156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/all_kindle_review.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Create and insert the Sentiment column based on rating\n",
        "# 1 for positive sentiment (rating > 3), 0 for negative sentiment (rating <= 3)\n",
        "df['Sentiment'] = df['rating'].apply(lambda x: 1 if x > 3 else 0)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(df[['rating', 'Sentiment']].head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WHBEagUqaGG",
        "outputId": "4267d315-5a25-49b2-9771-cf3599276b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   rating  Sentiment\n",
            "0       3          0\n",
            "1       5          1\n",
            "2       3          0\n",
            "3       3          0\n",
            "4       4          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z57NgXgGqElg",
        "outputId": "08e532cb-0072-4796-ad43-84312146a5e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          reviewText  \\\n",
            "0  Jace Rankin may be short, but he's nothing to ...   \n",
            "1  Great short read.  I didn't want to put it dow...   \n",
            "2  I'll start by saying this is the first of four...   \n",
            "3  Aggie is Angela Lansbury who carries pocketboo...   \n",
            "4  I did not expect this type of book to be in li...   \n",
            "\n",
            "                                Processed_ReviewText  \n",
            "0  jace rankin may short hes nothing mess man hau...  \n",
            "1  great short read didnt want put read one sitti...  \n",
            "2  ill start saying first four books wasnt expect...  \n",
            "3  aggie angela lansbury carries pocketbooks inst...  \n",
            "4  expect type book library pleased find price right  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab data package\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/all_kindle_review.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define a function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # 1. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove punctuation and special characters\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # 3. Optionally, remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)  # Tokenize the text\n",
        "    text = ' '.join([word for word in tokens if word not in stop_words])\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the 'reviewText' column\n",
        "df['Processed_ReviewText'] = df['reviewText'].astype(str).apply(preprocess_text)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(df[['reviewText', 'Processed_ReviewText']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE EXTRACTION USING TfidfVectorizer\n",
        "\n"
      ],
      "metadata": {
        "id": "_jPVxLGl2irM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/all_kindle_review.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Ensure preprocessing has already been applied to the reviewText column\n",
        "# If not, preprocess the text\n",
        "if 'Processed_ReviewText' not in df.columns:\n",
        "    df['Processed_ReviewText'] = df['reviewText'].astype(str).apply(preprocess_text)\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,  # Maximum number of features (vocabulary size)\n",
        "    ngram_range=(1, 2),  # Consider unigrams and bigrams\n",
        "    stop_words='english'  # Remove stop words\n",
        ")\n",
        "\n",
        "# Fit and transform the Processed_ReviewText column\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Processed_ReviewText'])\n",
        "\n",
        "# Convert the sparse matrix to a DataFrame for easier interpretation\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf_vectorizer.get_feature_names_out()\n",
        ")\n",
        "\n",
        "# Display the first few rows of the TF-IDF features\n",
        "print(\"TF-IDF Features:\")\n",
        "print(tfidf_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPaBlYWtw0He",
        "outputId": "379afe8a-cf9d-428c-df5f-f3bfa310b767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Features:\n",
            "   099   10  10 years  100  100 pages   11   12  12 stars   13   14  ...  \\\n",
            "0  0.0  0.0       0.0  0.0        0.0  0.0  0.0       0.0  0.0  0.0  ...   \n",
            "1  0.0  0.0       0.0  0.0        0.0  0.0  0.0       0.0  0.0  0.0  ...   \n",
            "2  0.0  0.0       0.0  0.0        0.0  0.0  0.0       0.0  0.0  0.0  ...   \n",
            "3  0.0  0.0       0.0  0.0        0.0  0.0  0.0       0.0  0.0  0.0  ...   \n",
            "4  0.0  0.0       0.0  0.0        0.0  0.0  0.0       0.0  0.0  0.0  ...   \n",
            "\n",
            "   youve read  yummy  zach  zane  zero  zombie  zombies  zone  zorn  zsadist  \n",
            "0         0.0    0.0   0.0   0.0   0.0     0.0      0.0   0.0   0.0      0.0  \n",
            "1         0.0    0.0   0.0   0.0   0.0     0.0      0.0   0.0   0.0      0.0  \n",
            "2         0.0    0.0   0.0   0.0   0.0     0.0      0.0   0.0   0.0      0.0  \n",
            "3         0.0    0.0   0.0   0.0   0.0     0.0      0.0   0.0   0.0      0.0  \n",
            "4         0.0    0.0   0.0   0.0   0.0     0.0      0.0   0.0   0.0      0.0  \n",
            "\n",
            "[5 rows x 5000 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Sentiment column\n",
        "df['Sentiment'] = df['rating'].apply(lambda x: 1 if x > 3 else 0)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = tfidf_matrix  # Features (TF-IDF matrix)\n",
        "y = df['Sentiment']  # Labels (new sentiment column)\n",
        "\n",
        "# Use train_test_split to create training and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Multinomial Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier on the training data\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict sentiment on the test data\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTAbD0_ptTXI",
        "outputId": "b19ce82b-6d7e-400d-cfb2-dd0988e79a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.82      0.82      1190\n",
            "           1       0.83      0.83      0.83      1210\n",
            "\n",
            "    accuracy                           0.83      2400\n",
            "   macro avg       0.83      0.83      0.83      2400\n",
            "weighted avg       0.83      0.83      0.83      2400\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 978  212]\n",
            " [ 205 1005]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE EXTRACTION USING Word2Vec\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "3QBonXWy2UTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Ensure preprocessing has been applied\n",
        "if 'Processed_ReviewText' not in df.columns:\n",
        "    df['Processed_ReviewText'] = df['reviewText'].astype(str).apply(preprocess_text)\n",
        "\n",
        "# Tokenize the preprocessed text for Word2Vec\n",
        "tokenized_reviews = df['Processed_ReviewText'].apply(lambda x: x.split())\n",
        "\n",
        "# Train a Word2Vec model\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences=tokenized_reviews,\n",
        "    vector_size=100,  # Embedding size\n",
        "    window=5,  # Context window size\n",
        "    min_count=1,  # Minimum word frequency\n",
        "    sg=0,  # Skip-gram (sg=1) or CBOW (sg=0)\n",
        "    workers=4,  # Number of threads\n",
        "    epochs=10  # Number of training iterations\n",
        ")\n",
        "\n",
        "# Create sentence embeddings by averaging word vectors\n",
        "def get_sentence_vector(sentence, model, vector_size):\n",
        "    words = [word for word in sentence if word in model.wv]\n",
        "    if len(words) == 0:\n",
        "        return np.zeros(vector_size)\n",
        "    return np.mean(model.wv[words], axis=0)\n",
        "\n",
        "df['Sentence_Vector'] = tokenized_reviews.apply(\n",
        "    lambda x: get_sentence_vector(x, word2vec_model, vector_size=100)\n",
        ")\n",
        "\n",
        "# Convert the sentence embeddings into a feature matrix\n",
        "X = np.vstack(df['Sentence_Vector'].values)\n",
        "y = df['Sentiment']  # Labels (assumes the Sentiment column exists)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train and Evaluate Naive Bayes Classifier\n",
        "# Using GaussianNB since Word2Vec vectors are continuous\n",
        "nb_classifier = GaussianNB()\n",
        "\n",
        "# Fit the classifier on the training data\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict sentiment on the test data\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe3qfXQQzYaS",
        "outputId": "0ac5b0da-1713-45e5-87ed-4320b499d975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.76\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.78      0.76      1190\n",
            "           1       0.77      0.73      0.75      1210\n",
            "\n",
            "    accuracy                           0.76      2400\n",
            "   macro avg       0.76      0.76      0.76      2400\n",
            "weighted avg       0.76      0.76      0.76      2400\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[932 258]\n",
            " [326 884]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gI6zF6AD2BWC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}